# prompt/stream_chat_app.py

import json
import logging
from typing import AsyncGenerator
from colorama import init
import httpx
import asyncio

from config.config import CLIENT_CONFIGS
from config.models import model_registry
from utils.chat_history import ChatHistory
from utils.message_builder import build_messages
from utils.print_messages_colored import print_messages_colored

# -----------------------------
# åˆå§‹åŒ– colorama
# -----------------------------
init(autoreset=True)

# -----------------------------
# æ—¥å¿—é…ç½®
# -----------------------------
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# -----------------------------
# å…¨å±€å˜é‡
# -----------------------------
chat_history = ChatHistory(max_entries=50)  # åªä¿ç•™æœ€è¿‘ 50 æ¡å¯¹è¯
MAX_HISTORY_ENTRIES = 1                     # æœ€è¿‘å‡ æ¡å¯¹è¯ä¼ ç»™æ¨¡å‹
SAVE_STORY_SUMMARY_ONLY = True              # åªä¿å­˜æ‘˜è¦ï¼Œé¿å…æ–‡ä»¶å¤ªå¤§
DEBUG_STREAM = False                        # æ˜¯å¦æ‰“å°åŸå§‹æµï¼Œè°ƒè¯•ç”¨


# -----------------------------
# ç»Ÿä¸€çš„æµè§£æå‡½æ•°
# -----------------------------
def parse_stream_chunk(data_str: str) -> str | None:
    """
    å…¼å®¹ OpenAI / Gemini / å…¶ä»–æµå¼è¿”å›æ ¼å¼ï¼Œè§£æå†…å®¹ç‰‡æ®µ1
    """
    try:
        chunk = json.loads(data_str)

        # OpenAI é£æ ¼
        if "choices" in chunk:
            choices = chunk.get("choices")
            # é˜²å¾¡æ€§åˆ¤æ–­ï¼šå¿…é¡»æ˜¯éç©ºåˆ—è¡¨
            if not isinstance(choices, list) or len(choices) == 0:
                logger.debug(f"[ç©ºæˆ–éæ³• choices] {chunk}")
                return None

            choice = choices[0]
            # æœ‰äº› chunk åªåŒ…å« finish_reasonï¼Œä¸åŒ…å« delta
            if "delta" not in choice:
                logger.debug(f"[æ—  delta å­—æ®µ] {chunk}")
                return None

            delta = choice.get("delta", {})
            return delta.get("content")

        # Gemini é£æ ¼
        elif "candidates" in chunk:
            candidates = chunk.get("candidates", [])
            if not isinstance(candidates, list) or len(candidates) == 0:
                logger.debug(f"[ç©º candidates] {chunk}")
                return None

            parts = candidates[0].get("content", {}).get("parts", [])
            return "".join(p.get("text", "") for p in parts if "text" in p)

        # å…¶ä»–æœªçŸ¥ç»“æ„
        logger.debug(f"[æœªçŸ¥ç»“æ„] {chunk}")
        return None

    except json.JSONDecodeError:
        logger.warning(f"æ— æ•ˆ JSON: {data_str}")
        return None
    except Exception as e:
        logger.warning(f"[parse_stream_chunk å¼‚å¸¸] {e} - åŸå§‹æ•°æ®: {data_str}")
        return None


# -----------------------------
# æµå¼è°ƒç”¨æ¨¡å‹ï¼ˆç»“æ„åŒ–è¾“å‡º + å¼‚å¸¸å¤„ç†ç»†åˆ†ï¼‰
# -----------------------------

async def execute_model_for_app(
    model_name: str,
    user_input: str,
    system_instructions: str,
    personas: list[str],
    web_input: str = "",
    nsfw: bool = True,
    stream: bool = False,  # æµå¼æˆ–éæµå¼
    # image: bool = False,  # â† æ–°å¢
) -> AsyncGenerator[dict, None]:
    """
    è°ƒç”¨æ¨¡å‹å¹¶è¿”å›ç»“æœï¼Œæ”¯æŒæµå¼å’Œéæµå¼
    æ¯æ¬¡ yield ä¸€ä¸ª dictï¼ŒåŒ…å«ï¼š
    { "type": "chunk", "content": "æ–‡æœ¬ç‰‡æ®µ" }
    { "type": "end", "full": "å®Œæ•´å›å¤" }
    { "type": "error", "error": "é”™è¯¯æè¿°" }
    """
    logger.info(f"[æ‰§è¡Œæ¨¡å‹] nsfw={nsfw}")

    # æ„å»º messages
    messages = build_messages(
        system_instructions,
        personas,
        chat_history,
        user_input,
        web_input,
        nsfw=nsfw,
        max_history_entries=MAX_HISTORY_ENTRIES,
    )
    print_messages_colored(messages)

    # æ¨¡å‹é…ç½®
    model_details = model_registry(model_name)
    payload = {"model": model_details["label"], "stream": stream, "messages": messages}
    client_settings = CLIENT_CONFIGS[model_details["client_name"]]
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {client_settings['api_key']}"}

    chunks = []
    got_done_flag = False

    # -----------------------------
    # ğŸ¨ å›¾ç‰‡ç”Ÿæˆåˆ†æ”¯
    # -----------------------------
    # if image:
    #     payload = {
    #         "prompt": user_input,
    #         "model": model_details.get("image_model", model_details["label"]),  # å…è®¸ä¸åŒå›¾ç‰‡æ¨¡å‹
    #     }
    #     image_url = client_settings["base_url"].replace("/v1/chat/completions", "/v1/images/generations")
    #
    #     try:
    #         async with httpx.AsyncClient(timeout=60) as client:
    #             response = await client.post(image_url, headers=headers, json=payload)
    #
    #         if response.status_code != 200:
    #             yield {"type": "error", "error": f"å›¾ç‰‡æ¥å£è¿”å›é200çŠ¶æ€ç : {response.status_code}"}
    #             return
    #
    #         data = response.json()
    #         if "data" in data and isinstance(data["data"], list) and len(data["data"]) > 0:
    #             img_data = data["data"][0]
    #             img_url = img_data.get("url") or img_data.get("b64_json")
    #             yield {"type": "image", "url": img_url}
    #         else:
    #             yield {"type": "error", "error": "å›¾ç‰‡æ¥å£è¿”å›æ— æ•ˆæ•°æ®"}
    #
    #     except Exception as e:
    #         yield {"type": "error", "error": f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}"}
    #     return

    try:
        async with httpx.AsyncClient(timeout=None) as client:
            if stream:
                # æµå¼æ¨¡å¼
                async with client.stream("POST", client_settings["base_url"], headers=headers, json=payload) as response:
                    if response.status_code != 200:
                        error_msg = f"æ¨¡å‹æ¥å£è¿”å›é200çŠ¶æ€ç : {response.status_code}"
                        logger.error(error_msg)
                        yield {"type": "error", "error": error_msg}
                        return

                    async for line in response.aiter_lines():
                        if not (line and line.startswith("data: ")):
                            continue
                        data_str = line[6:].strip()

                        if DEBUG_STREAM:
                            print(f"[DEBUG] åŸå§‹æµ: {data_str}")

                        if data_str == "[DONE]":
                            got_done_flag = True
                            break

                        delta_content = parse_stream_chunk(data_str)
                        if delta_content:
                            chunks.append(delta_content)
                            yield {"type": "chunk", "content": delta_content}

            else:
                # éæµå¼æ¨¡å¼
                response = await client.post(client_settings["base_url"], headers=headers, json=payload)
                if response.status_code != 200:
                    error_msg = f"æ¨¡å‹æ¥å£è¿”å›é200çŠ¶æ€ç : {response.status_code}"
                    logger.error(error_msg)
                    yield {"type": "error", "error": error_msg}
                    return

                data = response.json()
                # è§£æ OpenAI / Gemini é£æ ¼
                if "choices" in data and data["choices"]:
                    for choice in data["choices"]:
                        text = choice.get("message", {}).get("content") or choice.get("text") or ""
                        if text:
                            chunks.append(text)
                            yield {"type": "chunk", "content": text}
                got_done_flag = True

    except httpx.TimeoutException:
        error_msg = "[ç½‘ç»œè¶…æ—¶] æ¨¡å‹æ¥å£æœªå“åº”"
        logger.error(error_msg)
        yield {"type": "error", "error": error_msg}
        return
    except httpx.ConnectError:
        error_msg = "[è¿æ¥å¤±è´¥] æ— æ³•è¿æ¥åˆ°æ¨¡å‹æ¥å£"
        logger.error(error_msg)
        yield {"type": "error", "error": error_msg}
        return
    except httpx.RequestError as e:
        error_msg = f"[è¯·æ±‚é”™è¯¯] {e}"
        logger.error(error_msg)
        yield {"type": "error", "error": error_msg}
        return
    except Exception as e:
        error_msg = f"[æœªçŸ¥é”™è¯¯] {e}"
        logger.exception(error_msg)
        yield {"type": "error", "error": error_msg}
        return

    # ç­‰å¾…è¾“å‡ºç¼“å†²åˆ·æ–°
    await asyncio.sleep(0.05)

    if not got_done_flag:
        logger.warning("æµå¼ä¼ è¾“æœªæ£€æµ‹åˆ° [DONE]ï¼Œè¾“å‡ºå¯èƒ½ä¸å®Œæ•´")

    # ä¿å­˜å†å²
    full_response_text = "".join(chunks)
    if full_response_text.strip():
        if SAVE_STORY_SUMMARY_ONLY:
            summary = chat_history._extract_summary_from_assistant(full_response_text)
            if summary:
                chat_history.add_entry(user_input, summary)
        else:
            chat_history.add_entry(user_input, full_response_text)
        chat_history.save_history()

    # è¾“å‡ºæœ€ç»ˆå®Œæ•´å†…å®¹
    yield {"type": "end", "full": full_response_text}

